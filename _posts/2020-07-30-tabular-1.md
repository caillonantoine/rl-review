---
layout: post
title:  "Tabular methods: part 1"
---

# k-armed bandit

> Reinforcement learning is a computational approach to understanding and automating goal-directed learning and decision making. It is distinguished from other computational approaches by its emphasis on learning by an agent from direct interaction with its environment, without requiring exemplary supervision or complete models of the environment. In our opinion, reinforcement learning is the first field to seriously address the computational issues that arise when learning from interaction with an environment in order to achieve long-term goals.

Reinforcement learning is about finding the best **policy** (behavior) to apply to an **agent** interacting with an **environment**. We often swap "agent" and "policy". Given a state $$S_t$$, an agent perform an action $$A_t$$ yielding a new state $$S_{t+1}$$ and a reward $$R_t$$. During learning, we try to find the optimal policy $$p^*(A_t \vert S_t)$$ that maximises the reward $$R_t$$ at each timestep.

Lets consider a *k*-armed bandit problem. 

> You are faced repeatedly with a choice among *k* different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or  time steps.

Each action has an expected or mean reward: we call it its **value**.

$$
q_*(a) = \mathbb E [R_t \vert A_t = a]
$$

This is a fundamental notion in reinforcement learning, since the optimal policy can be defined as always taking the action that has the higher value. Since we don't know the true value of action *a*, we define the estimated value of action *a* at time step *t* as $$Q_t(a)$$.

> If you maintain estimates of the action values, then at any time step there is at least one action whose estimated value is greatest. We call these the greedy actions. When you select one of these actions, we say that you are exploiting your current knowledge of the values of the actions. If instead you select one of the nongreedy actions, then we say you are exploring, because this enables you to improve your estimate of the nongreedy actionâ€™s value.

An recurrent problem in RL is finding a good balance between **exploitation** and **exploration**. We define a **greedy** action selection as

$$
A_t = \underset{a}{\text{argmax}} ~ Q_t(a),
$$

and a $$\epsilon$$-greedy action selection as

$$
A_t = \left \{\begin{array}{c}
\underset{a}{\text{argmax}} ~ Q_t(a) ~ \text{with probability }1-\epsilon\\
\text{random} ~ A_t ~ \text{with probability } \epsilon
\end{array} \right.
$$

Using $$\epsilon$$-greedy action selection policies ensures that every action $$A_t$$ is selected an infinite number of times when $$t \rightarrow \infty$$. Hence we can write a learning algorithm that samples from the $$\epsilon$$-greedy policy, and update its estimate of the action value $$Q_t(a)$$ by averaging the rewards.

There are different initializations af the estimate, like *realistic* or *optimistic*, yielding different learning curves.